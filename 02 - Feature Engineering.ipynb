{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f98e7e",
   "metadata": {},
   "source": [
    "# Netflix movie recommendation engine\n",
    "Kaggle competition link: https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c4957",
   "metadata": {},
   "source": [
    "After doing some preprocessing, now we have 2 datasets in sparse matrix format. \n",
    "1. train_sparse_matrix\n",
    "2. test_sparse_matrix\n",
    "\n",
    "Both the data sets have 'user' as rows and 'movie' as columns and 'rating' as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea1a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime # To compute time taken wherever necessary\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c991375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sparse_matrix loaded!\n",
      "test_sparse_matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load train & test sparse matrix which we've created in previous file\n",
    "transformed_folder = 'F:/09 - Machine Learning Case Studies/01 - Netflix Movies Recommendation/Transformed Data'\n",
    "train_sparse_matrix = sparse.load_npz(transformed_folder+'/train_sparse_matrix.npz')\n",
    "print('train_sparse_matrix loaded!')\n",
    "test_sparse_matrix = sparse.load_npz(transformed_folder+'/test_sparse_matrix.npz')\n",
    "print('test_sparse_matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e698737",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024bd3a",
   "metadata": {},
   "source": [
    "#### 2.1 Basic features based on statistics\n",
    "Lets try to build some features that will be useful in modelling. Few such features could be:\n",
    "1. Average of all ratings given\n",
    "2. Average rating per user\n",
    "3. Average rating per movie\n",
    "\n",
    "We will create a dictionary train_averages which will store all these values. Like:<br>\n",
    "train_averages = <br>\n",
    "{<br>\n",
    "'global_average' : xyz,<br>\n",
    "'user' : { 'user1': abc, 'user2':pqr},<br>\n",
    "'movie' : { 'movie1': abc, 'movie2':pqr}<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399b1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_averages = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d775e6",
   "metadata": {},
   "source": [
    "#### Global average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42767a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'global_average': 3.582890686321557}\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Global average\n",
    "train_averages['global_average'] = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "print(train_averages)\n",
    "print(\"*\"*50)\n",
    "print('Time taken: {}'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06651212",
   "metadata": {},
   "source": [
    "#### Average rating per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03d81947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing average ratings per user...\n",
      "Done!\n",
      "**************************************************\n",
      "Time taken: 0:00:01.172182\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Average rating per user\n",
    "print('Computing average ratings per user...')\n",
    "sum_of_ratings_per_user = train_sparse_matrix.sum(axis=1).A1\n",
    "no_of_ratings_per_user = (train_sparse_matrix!=0).sum(axis=1).A1\n",
    "\n",
    "u,m = train_sparse_matrix.shape\n",
    "average_ratings = dict()\n",
    "for i in range(0,u):\n",
    "    if no_of_ratings_per_user[i]!=0:\n",
    "        average_ratings[i] = sum_of_ratings_per_user[i]/no_of_ratings_per_user[i]\n",
    "\n",
    "train_averages['user'] = average_ratings\n",
    "print('Done!')\n",
    "print(\"*\"*50)\n",
    "print('Time taken: {}'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78109320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating given by user 25 is: 3.5\n"
     ]
    }
   ],
   "source": [
    "print('Average rating given by user 25 is: {}'.format(train_averages['user'][25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadb4de",
   "metadata": {},
   "source": [
    "#### Average rating per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "115f07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing average ratings per movie...\n",
      "Done!\n",
      "**************************************************\n",
      "Time taken: 0:00:00.556723\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Average rating per user\n",
    "print('Computing average ratings per movie...')\n",
    "sum_of_ratings_per_movie = train_sparse_matrix.sum(axis=0).A1\n",
    "no_of_ratings_per_movie = (train_sparse_matrix!=0).sum(axis=0).A1\n",
    "\n",
    "u,m = train_sparse_matrix.shape\n",
    "average_ratings = dict()\n",
    "for i in range(0,m):\n",
    "    if no_of_ratings_per_movie[i]!=0:\n",
    "        average_ratings[i] = sum_of_ratings_per_movie[i]/no_of_ratings_per_movie[i]\n",
    "\n",
    "train_averages['movie'] = average_ratings\n",
    "print('Done!')\n",
    "print(\"*\"*50)\n",
    "print('Time taken: {}'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c131ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating given for movie 50 is: 2.9930795847750864\n"
     ]
    }
   ],
   "source": [
    "print('Average rating given for movie 50 is: {}'.format(train_averages['movie'][50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb69aaa",
   "metadata": {},
   "source": [
    "#### 2.2 Compute user-user similarity matrix\n",
    "\n",
    "We know that there are more than 400k users. If we try to compute similarity using cosine similarity, we will need to do (400k * 400k / 2) = 80 billions computations, which will take days to compute even though it will only perform on nonzeros. <br>\n",
    "If we try to reduce number of dimensions using PCA or SVD, it will take even more time as the matrix will become dense and the multiplication will be done for each feature as there will not be zero cells. <br><br>\n",
    "\n",
    "One of the ideas is to compute similarity at <b>run time</b>. Here, we will follow below startegy to compute user-user similarity:<br>\n",
    "1. We will compute similarity (top N) for given user (run time)\n",
    "2. Once we compute similarity for any user, we will store this data in our customized data structure so that we can retirve it whenever we want in future\n",
    "3. Now next time if we want to compute similarity for any user, first we will check if we have already computed for that particular user. If yes, then get it from out customized data structure. If not, compute the similarity and store it in our customized data structure.\n",
    "<br><br>\n",
    "Our customized data structure will be a <b>dictionary of dictionaries</b><br>\n",
    "{ 'user1': {'similar_user1' : value1 }, {'similar_user2' : value2 }, {'similar_user3' : value3 } <br>\n",
    "  'user2': {'similar_user1' : value1 }, {'similar_user2' : value2 }, {'similar_user3' : value3 } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d0ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbf298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b632486",
   "metadata": {},
   "source": [
    "#### 2.3 Compute movie-movie similarity matrix\n",
    "\n",
    "We know that there are around 17k movies in total. If we try to compute cosine similarity, we will need to do (17k * 17k / 2) = 144 millions computations, which would not take much longer. In this case, we can definately go with cosine similarity to get similar movies for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f05173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
